2018-10-16 22:26:04,276 - INFO - train.py[:62] - build dataloader done
2018-10-16 22:26:04,358 - INFO - train.py[:71] - Voxelnet(
  (feature_learnig): feature_learning_network(
    (vfe1): VFE(
      (fcn1): FCN(
        (conv1): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (fcn1): FCN(
      (conv1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (conv3d): Conv_Middle_layers(
    (conv1): Conv3d(
      (conv1): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2): Conv3d(
      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3): Conv3d(
      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 1, 1))
      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (_rpn): RPN(
    (conv1_1): Conv2d(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv1_2): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv1_3): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv1_4): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2_1): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2_2): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2_3): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2_4): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv2_5): Conv2d(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3_1): Conv2d(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3_2): Conv2d(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3_3): Conv2d(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3_4): Conv2d(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (conv3_5): Conv2d(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (deconv1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4), padding=(0, 1), output_padding=(0, 1))
    (deconv2): ConvTranspose2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(0, 1), output_padding=(0, 1))
    (deconv3): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (rpn_head): NaiveRpnHead(
      (conv3x3): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3x3): ReLU(inplace)
      (conv_cls): Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))
      (conv_loc): Conv2d(512, 14, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
2018-10-16 22:26:07,405 - INFO - train.py[:116] - Epoch: [0][0/5] Loss: 0.71238 (rpn_cls: 0.69919 rpn_loc: 0.01319 rpn_acc: 0.42578)
2018-10-16 22:26:07,751 - INFO - train.py[:116] - Epoch: [0][1/5] Loss: 0.62004 (rpn_cls: 0.60624 rpn_loc: 0.01380 rpn_acc: 0.99414)
2018-10-16 22:26:08,095 - INFO - train.py[:116] - Epoch: [0][2/5] Loss: 0.46982 (rpn_cls: 0.43417 rpn_loc: 0.03565 rpn_acc: 0.98633)
2018-10-16 22:26:08,439 - INFO - train.py[:116] - Epoch: [0][3/5] Loss: 0.30609 (rpn_cls: 0.24348 rpn_loc: 0.06262 rpn_acc: 0.98047)
2018-10-16 22:26:08,650 - INFO - train.py[:116] - Epoch: [0][4/5] Loss: 0.28258 (rpn_cls: 0.18499 rpn_loc: 0.09760 rpn_acc: 0.96484)
2018-10-16 22:26:09,205 - INFO - train.py[:116] - Epoch: [1][0/5] Loss: 0.03819 (rpn_cls: 0.02597 rpn_loc: 0.01222 rpn_acc: 0.99609)
2018-10-16 22:26:09,550 - INFO - train.py[:116] - Epoch: [1][1/5] Loss: 0.07227 (rpn_cls: 0.06047 rpn_loc: 0.01180 rpn_acc: 0.99414)
2018-10-16 22:26:09,893 - INFO - train.py[:116] - Epoch: [1][2/5] Loss: 0.23755 (rpn_cls: 0.17674 rpn_loc: 0.06081 rpn_acc: 0.98633)
2018-10-16 22:26:10,237 - INFO - train.py[:116] - Epoch: [1][3/5] Loss: 0.29336 (rpn_cls: 0.22788 rpn_loc: 0.06548 rpn_acc: 0.98047)
2018-10-16 22:26:10,449 - INFO - train.py[:116] - Epoch: [1][4/5] Loss: 0.37858 (rpn_cls: 0.30131 rpn_loc: 0.07727 rpn_acc: 0.96484)
2018-10-16 22:26:11,076 - INFO - train.py[:116] - Epoch: [2][0/5] Loss: 0.03704 (rpn_cls: 0.02544 rpn_loc: 0.01160 rpn_acc: 0.99609)
2018-10-16 22:26:11,422 - INFO - train.py[:116] - Epoch: [2][1/5] Loss: 0.07832 (rpn_cls: 0.05849 rpn_loc: 0.01984 rpn_acc: 0.99414)
2018-10-16 22:26:11,766 - INFO - train.py[:116] - Epoch: [2][2/5] Loss: 0.13368 (rpn_cls: 0.08449 rpn_loc: 0.04919 rpn_acc: 0.98633)
2018-10-16 22:26:12,111 - INFO - train.py[:116] - Epoch: [2][3/5] Loss: 0.18954 (rpn_cls: 0.12161 rpn_loc: 0.06793 rpn_acc: 0.98047)
2018-10-16 22:26:12,323 - INFO - train.py[:116] - Epoch: [2][4/5] Loss: 0.28422 (rpn_cls: 0.17334 rpn_loc: 0.11088 rpn_acc: 0.96484)
2018-10-16 22:26:12,921 - INFO - train.py[:116] - Epoch: [3][0/5] Loss: 0.10229 (rpn_cls: 0.08882 rpn_loc: 0.01346 rpn_acc: 0.99609)
2018-10-16 22:26:13,266 - INFO - train.py[:116] - Epoch: [3][1/5] Loss: 0.10946 (rpn_cls: 0.09519 rpn_loc: 0.01427 rpn_acc: 0.99414)
2018-10-16 22:26:13,612 - INFO - train.py[:116] - Epoch: [3][2/5] Loss: 0.12698 (rpn_cls: 0.09443 rpn_loc: 0.03254 rpn_acc: 0.98633)
2018-10-16 22:26:13,960 - INFO - train.py[:116] - Epoch: [3][3/5] Loss: 0.15404 (rpn_cls: 0.10285 rpn_loc: 0.05118 rpn_acc: 0.98047)
2018-10-16 22:26:14,172 - INFO - train.py[:116] - Epoch: [3][4/5] Loss: 0.22347 (rpn_cls: 0.14292 rpn_loc: 0.08055 rpn_acc: 0.96484)
2018-10-16 22:26:14,726 - INFO - train.py[:116] - Epoch: [4][0/5] Loss: 0.04707 (rpn_cls: 0.03688 rpn_loc: 0.01019 rpn_acc: 0.99609)
2018-10-16 22:26:15,072 - INFO - train.py[:116] - Epoch: [4][1/5] Loss: 0.05510 (rpn_cls: 0.04341 rpn_loc: 0.01169 rpn_acc: 0.99414)
2018-10-16 22:26:15,418 - INFO - train.py[:116] - Epoch: [4][2/5] Loss: 0.09701 (rpn_cls: 0.06581 rpn_loc: 0.03120 rpn_acc: 0.98633)
2018-10-16 22:26:15,764 - INFO - train.py[:116] - Epoch: [4][3/5] Loss: 0.13799 (rpn_cls: 0.09163 rpn_loc: 0.04635 rpn_acc: 0.98047)
2018-10-16 22:26:15,976 - INFO - train.py[:116] - Epoch: [4][4/5] Loss: 0.20873 (rpn_cls: 0.13679 rpn_loc: 0.07194 rpn_acc: 0.96484)
2018-10-16 22:26:16,557 - INFO - train.py[:116] - Epoch: [5][0/5] Loss: 0.04247 (rpn_cls: 0.03267 rpn_loc: 0.00980 rpn_acc: 0.99609)
2018-10-16 22:26:16,905 - INFO - train.py[:116] - Epoch: [5][1/5] Loss: 0.05143 (rpn_cls: 0.04095 rpn_loc: 0.01047 rpn_acc: 0.99414)
2018-10-16 22:26:17,250 - INFO - train.py[:116] - Epoch: [5][2/5] Loss: 0.09378 (rpn_cls: 0.06506 rpn_loc: 0.02872 rpn_acc: 0.98633)
2018-10-16 22:26:17,597 - INFO - train.py[:116] - Epoch: [5][3/5] Loss: 0.13435 (rpn_cls: 0.09174 rpn_loc: 0.04262 rpn_acc: 0.98047)
2018-10-16 22:26:17,810 - INFO - train.py[:116] - Epoch: [5][4/5] Loss: 0.20461 (rpn_cls: 0.13622 rpn_loc: 0.06838 rpn_acc: 0.96484)
2018-10-16 22:26:18,381 - INFO - train.py[:116] - Epoch: [6][0/5] Loss: 0.04199 (rpn_cls: 0.03246 rpn_loc: 0.00953 rpn_acc: 0.99609)
2018-10-16 22:26:18,727 - INFO - train.py[:116] - Epoch: [6][1/5] Loss: 0.04852 (rpn_cls: 0.03931 rpn_loc: 0.00921 rpn_acc: 0.99414)
2018-10-16 22:26:19,074 - INFO - train.py[:116] - Epoch: [6][2/5] Loss: 0.09043 (rpn_cls: 0.06330 rpn_loc: 0.02713 rpn_acc: 0.98633)
2018-10-16 22:26:19,422 - INFO - train.py[:116] - Epoch: [6][3/5] Loss: 0.13215 (rpn_cls: 0.09074 rpn_loc: 0.04141 rpn_acc: 0.98047)
2018-10-16 22:26:19,635 - INFO - train.py[:116] - Epoch: [6][4/5] Loss: 0.20344 (rpn_cls: 0.13510 rpn_loc: 0.06834 rpn_acc: 0.96484)
2018-10-16 22:26:20,238 - INFO - train.py[:116] - Epoch: [7][0/5] Loss: 0.04286 (rpn_cls: 0.03339 rpn_loc: 0.00947 rpn_acc: 0.99609)
2018-10-16 22:26:20,585 - INFO - train.py[:116] - Epoch: [7][1/5] Loss: 0.04864 (rpn_cls: 0.03958 rpn_loc: 0.00906 rpn_acc: 0.99414)
2018-10-16 22:26:20,934 - INFO - train.py[:116] - Epoch: [7][2/5] Loss: 0.09107 (rpn_cls: 0.06432 rpn_loc: 0.02675 rpn_acc: 0.98633)
2018-10-16 22:26:21,278 - INFO - train.py[:116] - Epoch: [7][3/5] Loss: 0.13123 (rpn_cls: 0.09000 rpn_loc: 0.04123 rpn_acc: 0.98047)
2018-10-16 22:26:21,493 - INFO - train.py[:116] - Epoch: [7][4/5] Loss: 0.20381 (rpn_cls: 0.13556 rpn_loc: 0.06825 rpn_acc: 0.96484)
2018-10-16 22:26:22,074 - INFO - train.py[:116] - Epoch: [8][0/5] Loss: 0.04051 (rpn_cls: 0.03108 rpn_loc: 0.00943 rpn_acc: 0.99609)
2018-10-16 22:26:22,420 - INFO - train.py[:116] - Epoch: [8][1/5] Loss: 0.05109 (rpn_cls: 0.04221 rpn_loc: 0.00887 rpn_acc: 0.99414)
2018-10-16 22:26:22,766 - INFO - train.py[:116] - Epoch: [8][2/5] Loss: 0.08997 (rpn_cls: 0.06359 rpn_loc: 0.02638 rpn_acc: 0.98633)
2018-10-16 22:26:23,113 - INFO - train.py[:116] - Epoch: [8][3/5] Loss: 0.13120 (rpn_cls: 0.08997 rpn_loc: 0.04123 rpn_acc: 0.98047)
2018-10-16 22:26:23,325 - INFO - train.py[:116] - Epoch: [8][4/5] Loss: 0.20025 (rpn_cls: 0.13256 rpn_loc: 0.06768 rpn_acc: 0.96484)
2018-10-16 22:26:23,888 - INFO - train.py[:116] - Epoch: [9][0/5] Loss: 0.04130 (rpn_cls: 0.03191 rpn_loc: 0.00939 rpn_acc: 0.99609)
2018-10-16 22:26:24,234 - INFO - train.py[:116] - Epoch: [9][1/5] Loss: 0.04805 (rpn_cls: 0.03915 rpn_loc: 0.00889 rpn_acc: 0.99414)
2018-10-16 22:26:24,579 - INFO - train.py[:116] - Epoch: [9][2/5] Loss: 0.08911 (rpn_cls: 0.06279 rpn_loc: 0.02632 rpn_acc: 0.98633)
2018-10-16 22:26:24,928 - INFO - train.py[:116] - Epoch: [9][3/5] Loss: 0.13081 (rpn_cls: 0.08979 rpn_loc: 0.04102 rpn_acc: 0.98047)
2018-10-16 22:26:25,141 - INFO - train.py[:116] - Epoch: [9][4/5] Loss: 0.20121 (rpn_cls: 0.13362 rpn_loc: 0.06759 rpn_acc: 0.96484)
